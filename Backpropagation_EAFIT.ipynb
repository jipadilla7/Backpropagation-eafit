{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Neural Networks and Backpropagation: An In-Depth Exploration of Multivariate Functions**\n",
        "\n",
        "Author:\n",
        "\n",
        "Jorge Iván Padilla Buriticá, PhD.\n",
        "\n",
        "Contact:\n",
        "\n",
        "Email: jorgeivanpb@gmail.com\n",
        "\n",
        "**Contents:**\n",
        "\n",
        "This notebook explores detailed exercises involving forward and backward propagation using different multivariate functions. Each exercise covers mathematical analysis, gradient computation, and practical gradient descent applications for neural network training."
      ],
      "metadata": {
        "id": "jT7R_3hykssP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multivariate Functions and Neural Network Computation (Forward and Backward Propagation)\n",
        "\n",
        "In this notebook, we illustrate two examples of multivariate functions that demonstrate the concepts of forward propagation and backpropagation.\n",
        "\n",
        "### Forward Propagation\n",
        "Forward propagation is the computational process in which input values pass through a network or computational graph to produce output predictions. The function output is calculated by applying operations layer by layer.\n",
        "\n",
        "### Backpropagation\n",
        "Backpropagation is a method used primarily in training neural networks, allowing the network to adjust its internal parameters (weights) by calculating gradients of the loss function with respect to these parameters. It involves propagating error gradients backward through the network.\n",
        "\n",
        "### Examples\n",
        "\n",
        "#### Example 1: Linear-Combination Function\n",
        "\n",
        "- **Function:**\n",
        "\n",
        "\n",
        "   $$\n",
        "   f(x, y, z) = (x + y) \\times z\n",
        "   $$\n",
        "\n",
        "- **Forward propagation:** Compute the intermediate and final outputs using provided inputs.\n",
        "- **Backpropagation:** Compute gradients of the function with respect to each input variable \\( (x, y, z) \\).\n",
        "\n",
        "#### Example 2: Sigmoid Activation Function\n",
        "\n",
        "- **Function:**\n",
        "\n",
        "$$\n",
        "f(w,x) = \\frac{1}{1 + e^{-(w_0 x_0 + w_1 x_1 + w_2)}}\n",
        "$$\n",
        "\n",
        "- **Forward propagation:** Calculate the output using the sigmoid activation based on given inputs and weights.\n",
        "- **Backpropagation:** Determine the gradients with respect to each weight \\( w_0, w_1, w_2 \\) to measure sensitivity.\n",
        "\n",
        "### Interpretation of Outputs and Gradients\n",
        "- **Output Interpretation (Forward Propagation):** The outputs represent predictions or calculated values from the given inputs based on the computational graph.\n",
        "- **Gradient Interpretation (Backward Propagation):** Gradients indicate the sensitivity of the function’s output with respect to changes in each input or parameter. A higher absolute gradient value implies greater sensitivity, guiding how parameter updates should be adjusted during optimization.\n",
        "\n"
      ],
      "metadata": {
        "id": "_ZNQpA8LXAGY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uKWddZRKR1Pn",
        "outputId": "77c50d1c-6b86-4436-cd49-ccf0fb4c8fc2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Forward propagation: f = -12\n",
            "Forward propagation: q = 3\n",
            "Backward propagation (gradients):\n",
            "df/dx = -4\n",
            "df/dy = -4\n",
            "df/dz = 3\n"
          ]
        }
      ],
      "source": [
        "# Implementation of the multivariate function f(x, y, z) = (x + y) * z\n",
        "# with forward and backward propagation\n",
        "\n",
        "# Forward propagation\n",
        "def forward(x, y, z):\n",
        "    q = x + y\n",
        "    f = q * z\n",
        "    return f, q\n",
        "\n",
        "# Backward propagation\n",
        "def backward(q, z):\n",
        "    # partial derivatives\n",
        "    df_dz = q          # ∂f/∂z = q\n",
        "    df_dq = z          # ∂f/∂q = z\n",
        "\n",
        "    # ∂q/∂x = 1, ∂q/∂y = 1\n",
        "    df_dx = df_dq * 1  # ∂f/∂x = z\n",
        "    df_dy = df_dq * 1  # ∂f/∂y = z\n",
        "\n",
        "    return df_dx, df_dy, df_dz\n",
        "\n",
        "# Example with provided values:\n",
        "x, y, z = -2, 5, -4\n",
        "\n",
        "# Forward propagation\n",
        "f, q = forward(x, y, z)\n",
        "print(f\"Forward propagation: f = {f}\")\n",
        "print(f\"Forward propagation: q = {q}\")\n",
        "\n",
        "# Backward propagation\n",
        "df_dx, df_dy, df_dz = backward(q, z)\n",
        "print(f\"Backward propagation (gradients):\")\n",
        "print(f\"df/dx = {df_dx}\")\n",
        "print(f\"df/dy = {df_dy}\")\n",
        "print(f\"df/dz = {df_dz}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Interpretation of Forward and Backward Propagation Results (Exercise 01):\n",
        "\n",
        "#### Function:\n",
        "$$\n",
        "f(x, y, z) = (x + y) \\times z   = q \\times z\n",
        "$$\n",
        "\n",
        "#### Forward Propagation Results:\n",
        "- **Intermediate Calculation:**\n",
        "$$\n",
        "q = 3\n",
        "$$\n",
        "This value indicates the intermediate result, \\( q = x + y \\). With the given inputs, this sum equals **3**.\n",
        "  \n",
        "- **Final Output:**  \n",
        "$$\n",
        "f = -12\n",
        "$$\n",
        "The final result is calculated by multiplying\n",
        "$$\n",
        "f = q \\times z = -12\n",
        "$$\n",
        "\n",
        "#### Backward Propagation (Gradients) Interpretation:\n",
        "The gradients illustrate the sensitivity of the output to small changes in each input variable:\n",
        "\n",
        "- **Gradient with respect to $ x $:\n",
        "$$\n",
        " \\frac{\\partial f}{\\partial x} = -4\n",
        " $$\n",
        "\n",
        "Increasing $x$ slightly results in decreasing the output $f$ by approximately 4 times the increment.\n",
        "\n",
        "- **Gradient with respect to $y$:\n",
        "$$\n",
        "\\frac{\\partial f}{\\partial y} = -4\n",
        "$$\n",
        "\n",
        "Similarly, increasing $ y $ slightly will also decrease the output $ f $ by roughly 4 times that increment. Thus, $x$ and $ y$ equally impact the output.\n",
        "\n",
        "- **Gradient with respect to $ z$**:\n",
        "$$\n",
        "  \\frac{\\partial f}{\\partial z} = 3\n",
        " $$\n",
        "An incremental increase in $z$ will increase the output $f$ approximately 3 times that increment, indicating a direct proportional relationship.\n",
        "\n",
        "These gradients help identify how sensitive the function output is to each input, aiding in optimization and parameter adjustments during model training.\n"
      ],
      "metadata": {
        "id": "ZQH4SsOxYZUn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How to Use Gradient Values for Updating Weights in Neural Networks (Backpropagation)\n",
        "\n",
        "In neural network training, gradients computed through **backpropagation** are essential for updating the network's weights. These gradients represent the sensitivity of the output with respect to each weight and guide the adjustments needed to minimize the error or loss function.\n",
        "\n",
        "### General Weight Update Rule (Gradient Descent)\n",
        "The general formula for updating weights using gradients is given by:\n",
        "\n",
        "$$\n",
        "w_{\\text{new}} = w_{\\text{old}} + \\Delta w \\quad\\text{where}\\quad \\Delta w = -\\alpha \\frac{\\partial E}{\\partial w}\n",
        "$$\n",
        "\n",
        "- $ w_{\\text{old}} $: Current value of the weight.\n",
        "- $ \\alpha$: Learning rate, a small positive constant (typically between 0.001 and 0.1).\n",
        "- Gradient: The partial derivative of the loss function with respect to the weight.\n",
        "\n",
        "### Example Using Gradients in our function:\n",
        "Consider the function:\n",
        "$$\n",
        "f(x, y, z) = (x + y) \\times z\n",
        "$$\n",
        "Suppose we obtained the following gradients:\n",
        "\n",
        "- $\\frac{\\partial f}{\\partial x} = -4$\n",
        "- $\\frac{\\partial f}{\\partial y} = -4$\n",
        "- $\\frac{\\partial f}{\\partial z} = 3$\n",
        "\n",
        "### Practical Example of Weight Updates:\n",
        "\n",
        "Let's assume the following initial weights (or inputs, considered analogous to network weights):\n",
        "- $w_x = 2$, $w_y = 5$, $w_z = -4$\n",
        "\n",
        "Using a learning rate $(\\alpha$) of 0.1, we update the weights as follows:\n",
        "\n",
        "| Weight | Current Value | Gradient | Update Calculation                         | Updated Value |\n",
        "|--------|---------------|----------|--------------------------------------------|---------------|\n",
        "| $w_x$| 2             | -4       | $2 - (0.1 \\times -4) = 2 + 0.4$        | **2.4**       |\n",
        "| $w_y$| 5             | -4       | $5 - (0.1 \\times -4) = 5 + 0.4$        | **5.4**       |\n",
        "| $w_z$| -4            | 3        | $-4 - (0.1 \\times 3) = -4 - 0.3$       | **-4.3**      |\n",
        "\n",
        "### Interpretation:\n",
        "- **Negative gradient:** Suggests increasing the weight will decrease the error or loss.\n",
        "- **Positive gradient:** Suggests decreasing the weight will reduce the error or loss.\n",
        "\n",
        "This iterative adjustment using gradients systematically guides the neural network towards optimal weight values, minimizing the loss function and enhancing overall model performance.\n",
        "\n",
        "\n",
        "## Activation Functions in Neural Networks\n",
        "\n",
        "### Overview\n",
        "Activation functions play a crucial role in neural network modeling by introducing non-linear transformations. These functions determine the output of a neuron, influencing the network's capability to learn and generalize complex data patterns.\n",
        "\n",
        "### Importance of Activation Functions\n",
        "Activation functions are important because they:\n",
        "- Introduce non-linearity, enabling neural networks to approximate complex, non-linear relationships.\n",
        "- Determine neuronal firing based on inputs, guiding decision-making within the network.\n",
        "- Facilitate effective gradient flow during backpropagation, crucial for accurate training and convergence.\n",
        "\n",
        "### Activation Function in the Provided Example\n",
        "Consider the following example function:\n",
        "\n",
        "$$\n",
        "f(x, y, z) = (x + y) \\times z\n",
        "$$\n",
        "\n",
        "In this example, the activation function implicitly used is a **linear activation** function:\n",
        "\n",
        "$$\n",
        "g(q) = q \\quad \\text{with} \\quad q = (x + y)\n",
        "$$\n",
        "\n",
        "### Characteristics of the Linear Activation Function\n",
        "- **Linear behavior:** Does not alter input data beyond scaling, offering no non-linear transformation.\n",
        "- **Limitations:** Unable to capture complex, non-linear relationships between variables effectively.\n",
        "- **Applications:** Suitable primarily for regression tasks or as the final output layer in neural networks when continuous outputs are needed.\n",
        "\n",
        "### Modifying the Activation Function\n",
        "To enhance the capability of this model, a non-linear activation function could be employed. Common choices include:\n",
        "\n",
        "- **Sigmoid:**\n",
        "  $$\n",
        "  g(q) = \\frac{1}{1 + e^{-q}}\n",
        "  $$\n",
        "\n",
        "- **Rectified Linear Unit (ReLU):**\n",
        "  $$\n",
        "  g(q) = \\text{max}(0, q)\n",
        "  $$\n",
        "\n",
        "- **Hyperbolic Tangent (Tanh):**\n",
        "  $$\n",
        "  g(q) = \\frac{e^{q} - e^{-q}}{e^{q} + e^{-q}}\n",
        "  $$\n",
        "\n",
        "### Example of Activation Function Modification\n",
        "You can easily integrate a non-linear activation function as follows:\n",
        "\n",
        "- Original computation:\n",
        "  $ q = x + y $\n",
        "\n",
        "- Modified with sigmoid activation:\n",
        "  $ q = \\frac{1}{1 + e^{-(x+y)}} $\n",
        "\n",
        "### Conclusion\n",
        "Initially, the provided example implicitly uses a linear activation function. Introducing a non-linear activation can significantly enhance the neural network’s ability to model complex and realistic data scenarios, facilitating improved learning outcomes.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pZD669kua4Ff"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Sigmoid function\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "# Forward propagation\n",
        "def forward(w, x):\n",
        "    z = np.dot(w, x)\n",
        "    f = sigmoid(z)\n",
        "    return f, z\n",
        "\n",
        "# Backward propagation (gradient calculation)\n",
        "def backward(f, z, x):\n",
        "    # Derivative of sigmoid\n",
        "    df_dz = f * (1 - f)\n",
        "\n",
        "    # Gradient with respect to weights\n",
        "    df_dw = df_dz * x\n",
        "    return df_dw\n",
        "\n",
        "# Example inputs\n",
        "w = np.array([2, -3, -3])\n",
        "x = np.array([-1, -2, 1])  # x2 is assumed to be 1 for bias\n",
        "\n",
        "# Forward propagation\n",
        "f, z = forward(w, x)\n",
        "print(f\"Forward propagation result: f = {f}\")\n",
        "\n",
        "# Backward propagation\n",
        "grad_w = backward(f, z, x)\n",
        "print(f\"Backward propagation gradients: df/dw = {grad_w}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Su8-wfjQTpmn",
        "outputId": "e6a31d02-e9bf-4e6d-fe12-57fe7c3765a1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Forward propagation result: f = 0.7310585786300049\n",
            "Backward propagation gradients: df/dw = [-0.19661193 -0.39322387  0.19661193]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Activation Functions and Gradient Computation (Exercise 2)\n",
        "\n",
        "### Overview of the Exercise\n",
        "This exercise demonstrates forward and backward propagation through a neural network using a sigmoid activation function. The purpose is to understand how to compute predictions, gradients, and update weights effectively.\n",
        "\n",
        "### Sigmoid Activation Function\n",
        "The sigmoid function is defined as:\n",
        "\n",
        "$$\n",
        "g(z) = \\frac{1}{1 + e^{-z}}\n",
        "$$\n",
        "\n",
        "### Importance of the Sigmoid Function:\n",
        "- Introduces non-linearity, enabling the network to capture complex patterns.\n",
        "- Constrains outputs between 0 and 1, making it suitable for binary classification tasks.\n",
        "\n",
        "### Forward Propagation Explanation\n",
        "Given weights $ w $ and input values $x $, forward propagation computes:\n",
        "\n",
        "$$\n",
        "z = w_0 x_0 + w_1 x_1 + w_2 x_2\n",
        "$$\n",
        "\n",
        "Then, applies the sigmoid activation to obtain the output $ f $:\n",
        "\n",
        "$$\n",
        "f = \\frac{1}{1 + e^{-z}}\n",
        "$$\n",
        "\n",
        "### Interpretation of Forward Propagation\n",
        "- **$f$**: The predicted probability (between 0 and 1) indicating the network's output based on the inputs and current weights.\n",
        "- **$z$**: The linear combination of inputs and weights before activation.\n",
        "\n",
        "### Backward Propagation Explanation (Gradient Calculation)\n",
        "Backward propagation calculates gradients to measure how the output changes with respect to weights, enabling the network to adjust weights effectively.\n",
        "\n",
        "- Gradient of sigmoid:\n",
        "  $$\n",
        "  \\frac{\\partial f}{\\partial z} = f(1 - f)\n",
        "  $$\n",
        "\n",
        "- Gradient with respect to weights:\n",
        "  $$\n",
        "  \\frac{\\partial f}{\\partial w} = \\frac{\\partial f}{\\partial z} \\cdot x\n",
        "  $$\n",
        "\n",
        "### Interpretation of Gradients\n",
        "The computed gradients indicate how sensitive the network’s output is to each weight. Larger gradients imply greater influence and thus a stronger adjustment during updates.\n",
        "\n",
        "### Example Results (Your Exercise)\n",
        "- Forward propagation result (predicted output): $ f \\approx 0.731 $ indicates a high activation given the current inputs.\n",
        "- Gradients: Indicate the sensitivity of output concerning each weight, determining adjustment direction and magnitude:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial f}{\\partial w} = [df/dw_0, df/dw_1, df/dw_2]\n",
        "$$\n",
        "\n",
        "### Updating Weights Using Gradients\n",
        "The gradients help update the weights through gradient descent:\n",
        "\n",
        "$$\n",
        "w_{\\text{new}} = w_{\\text{old}} - \\alpha \\times \\frac{\\partial f}{\\partial w}\n",
        "$$\n",
        "\n",
        "- $ \\alpha $ (Learning rate): Typically a small value controlling the step size.\n",
        "- Negative gradient: Indicates the weight should increase.\n",
        "- Positive gradient: Indicates the weight should decrease.\n",
        "\n",
        "### Practical Example (Weight Update)\n",
        "Given your computed gradients and an example learning rate (e.g., 0.1), weights are updated:\n",
        "\n",
        "$$\n",
        "w_{\\text{new}} = [w_0, w_1, w_2] - 0.1 \\times \\frac{\\partial f}{\\partial w}\n",
        "$$\n",
        "\n",
        "### Conclusion\n",
        "This exercise demonstrates clearly how forward and backward propagation work together to predict outputs and iteratively adjust weights, ensuring improved performance of neural network models.\n",
        "\n"
      ],
      "metadata": {
        "id": "Vu7b7pDhfl7Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Proposed exercises on forward and backward propagation"
      ],
      "metadata": {
        "id": "OwROCLhYflZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercises for Neural Networks Using Forward and Backward Propagation\n",
        "\n",
        "### **Exercise 1**\n",
        "\n",
        "**Function:**\n",
        "$$\n",
        "f(x, y, z) = \\frac{x + y}{z}\n",
        "$$\n",
        "\n",
        "#### Instructions:\n",
        "- Implement forward propagation to compute $ f$.\n",
        "- Derive and implement backward propagation to compute gradients with respect to each input $ x, y, z $.\n",
        "- Use these gradients to update weights using gradient descent.\n",
        "\n",
        "#### Example Inputs:\n",
        "- $x = 3, \\quad y = 2, \\quad z = 1 $\n",
        "\n",
        "#### Mathematical Analysis Questions:\n",
        "- How do gradients behave as $ z $ approaches zero?\n",
        "- What implications do these gradient behaviors have on network stability during training?\n",
        "\n",
        "### **Exercise 2**\n",
        "**Function:**\n",
        "\n",
        "$$\n",
        "f(x, y, z) = x e^{y} + \\sin(z)\n",
        "$$\n",
        "\n",
        "#### Instructions:\n",
        "- Compute forward propagation to determine the output.\n",
        "- Calculate backward propagation gradients explicitly for each variable $ x, y, z $.\n",
        "- Demonstrate weight updates using gradient descent.\n",
        "\n",
        "#### Example Inputs:\n",
        "- $$\n",
        " x = 1.5, \\quad y = -1, \\quad z = \\frac{\\pi}{2}\n",
        " $$\n",
        "\n",
        "#### Mathematical Analysis Questions:\n",
        "- Which input has the greatest effect on the function's output based on gradient magnitudes?\n",
        "- How do exponential and trigonometric functions impact gradient stability?\n",
        "\n",
        "### **Exercise 3:**\n",
        "**Function:**\n",
        "\n",
        "$$\n",
        "f(x, y) = \\log(x^{2} + y^{2} + 1)\n",
        "$$\n",
        "\n",
        "#### Instructions:\n",
        "- Perform forward propagation for output calculation.\n",
        "- Compute backward propagation gradients explicitly for \\( x \\) and \\( y \\).\n",
        "- Apply computed gradients in gradient descent to update the inputs or weights.\n",
        "\n",
        "#### Example Inputs:\n",
        "- $$\n",
        " x = 2, \\quad y = 1\n",
        " $$\n",
        "\n",
        "#### Mathematical Analysis Questions:\n",
        "- How do gradients behave as inputs become very large or approach zero?\n",
        "- Discuss techniques for maintaining numerical stability during backpropagation with logarithmic functions.\n",
        "\n",
        "---\n",
        "\n",
        "### General Gradient Descent Update Rule (Common to All Exercises):\n",
        "\n",
        "$$\n",
        "w_{\\text{new}} = w_{\\text{old}} - \\alpha \\frac{\\partial f}{\\partial w}\n",
        "$$\n",
        "\n",
        "- $ \\alpha $: Learning rate (typically a small positive value).\n",
        "- Interpretation: Gradients guide weight updates, improving the neural network's performance by minimizing errors.\n",
        "\n"
      ],
      "metadata": {
        "id": "EyceHEKwi58v"
      }
    }
  ]
}